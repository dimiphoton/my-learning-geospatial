class: middle, center, title-slide

# About me

Lecture 0: Introduction

<br><br>
Dimitri MARCHAND<br>
[pro.dimitrimarchand@gmail.com](mailto:pro.dimitrimarchand@gmail.com)

???

- Universal approximation theorem? + magic trick
- https://arxiv.org/abs/2105.04026?s=03 
- Remove lecture 12 on Deep RL

---

# Today

- Course outline
- Introduction to deep learning
- Fundamentals of machine learning

---

# Outline

- Lecture 0: Introduction
- Lecture 1: Fundamentals of machine learning
- Lecture 2: Multi-layer perceptron
- Lecture 3: Automatic differentiation
- Lecture 4: Training neural networks
- Lecture 5: Convolutional neural networks
- Lecture 6: Computer vision
- Lecture 7: Recurrent neural networks
- Lecture 8: Attention and transformer networks
- Lecture 9: Generative models (Part 1)
- Lecture 10: Generative models (Part 2)
- Lecture 11: Uncertainty
- Lecture 12: Deep reinforcement learning

---

class: middle

.center.width-60[![](figures/lec0/map.png)]

---

class: middle

## My mission

By the end of this course, you will have acquired a solid and detailed understanding of the field of deep learning. 

You will have learned how to design deep neural networks for a wide range of advanced probabilistic inference tasks and how to train them.

These models seen in the course apply to a wide variety of artificial intelligence problems, with plenty of applications in engineering and science.

---

class: middle

# Why learning?

---

class: middle, center

.width-100[![](figures/lec0/mushrooms.png)]

What do you see?

???

.italic[How do you do that?]

---

class: middle

.center[
.width-70[![](figures/lec0/dog1.jpg)]

Sheepdog or mop?
]

.footnote[Credits: [Karen Zack](https://twitter.com/teenybiscuit), 2016.]

---
